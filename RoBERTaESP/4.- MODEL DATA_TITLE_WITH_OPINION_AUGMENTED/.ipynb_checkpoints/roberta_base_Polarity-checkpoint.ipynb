{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POLARITY - ROBERTA BASE - DATASET AUGMENTED - TITTLE + OPINION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "# tf.config.set_logical_device_configuration(\n",
    "# gpus[0],\n",
    "# [tf.config.LogicalDeviceConfiguration(memory_limit=9216  )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"PlanTL-GOB-ES/roberta-base-bne\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5,\n",
    "                                                             from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration javilonso--mex_data_title_with_opinion_augmented-e715704dbb548eb1\n",
      "Reusing dataset parquet (/home/javilonso/.cache/huggingface/datasets/parquet/javilonso--mex_data_title_with_opinion_augmented-e715704dbb548eb1/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f977b498a34617a785c00a38fab0a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "raw_dataset = load_dataset(\"javilonso/mex_data_title_with_opinion_augmented\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['Title', 'Opinion', 'Polarity', 'Attraction', 'Title_Opinion', '__index_level_0__'],\n",
       "        num_rows: 7690\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['Title', 'Opinion', 'Polarity', 'Attraction', 'Title_Opinion', '__index_level_0__'],\n",
       "        num_rows: 30756\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef0706f970a4e2e9d8eba521f32956b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24df11d080674a94b6a817c1cbb70179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(example):\n",
    "    tokenized_example = tokenizer(example[\"Title_Opinion\"], truncation=True)\n",
    "    tokenized_example[\"label\"] = example[\"Polarity\"]\n",
    "    return tokenized_example\n",
    "\n",
    "\n",
    "tokenized_dataset = raw_dataset.map(tokenize, batched=True, remove_columns=raw_dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "tf_train_dataset = tokenized_dataset[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"label\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "tf_eval_dataset = tokenized_dataset[\"test\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"label\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: GeForce RTX 3060, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! Please ensure your labels are passed as keys in the input dict so that they are accessible to the model during the forward pass. To disable this behaviour, please pass a loss argument, or explicitly pass loss=None if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "from transformers import create_optimizer\n",
    "\n",
    "num_epochs = 2\n",
    "num_train_steps = len(tf_train_dataset) * num_epochs\n",
    "\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_warmup_steps=0,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    ")\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/javilonso/Mex_Rbta_TitleWithOpinion_Augmented_Polarity into local empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "  58/3844 [..............................] - ETA: 18:31 - loss: 1.1609"
     ]
    }
   ],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "callback = PushToHubCallback(output_dir=\"Mex_Rbta_TitleWithOpinion_Augmented_Polarity\", tokenizer=tokenizer)\n",
    "\n",
    "model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_eval_dataset,\n",
    "    callbacks=[callback],\n",
    "    epochs=num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from datasets import load_metric\n",
    " \n",
    "# load_accuracy = load_metric(\"accuracy\")\n",
    "# load_f1 = load_metric(\"f1\")\n",
    "# for batch in tf_eval_dataset:\n",
    "#     logits = model.predict(batch)[\"logits\"]\n",
    "#     labels = batch[\"labels\"]\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     load_accuracy.add_batch(predictions=predictions, references=labels)\n",
    "#     load_f1.add_batch(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "# accuracy = load_accuracy.compute()[\"accuracy\"]\n",
    "# f1 = load_f1.compute(average=None)[\"f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Accuracy: {accuracy:.2f}')\n",
    "# print(f'F1: {f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------- SKLEARN METRICS -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.74      0.79       540\n",
      "           1       0.62      0.65      0.64       586\n",
      "           2       0.75      0.77      0.76      1269\n",
      "           3       0.55      0.51      0.53      1181\n",
      "           4       0.88      0.90      0.89      4114\n",
      "\n",
      "    accuracy                           0.79      7690\n",
      "   macro avg       0.73      0.72      0.72      7690\n",
      "weighted avg       0.79      0.79      0.79      7690\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "y_test_aux = []\n",
    "y_pred_aux = []\n",
    "for batch in tf_eval_dataset:\n",
    "    logits = model.predict(batch)[\"logits\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    y_test_aux.append(labels)\n",
    "    y_pred_aux.append(predictions)\n",
    "\n",
    "# Flatten arrays\n",
    "y_pred = []\n",
    "for arr in y_pred_aux:\n",
    "    for elem in arr:\n",
    "        y_pred.append(elem)\n",
    "y_pred = np.array(y_pred)\n",
    "        \n",
    "y_test = []\n",
    "for arr in y_test_aux:\n",
    "    for elem in arr:\n",
    "        y_test.append(elem)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# TEST Predictions\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "# model_name = \"classificationEsp1_TitleWithOpinion_Polarity\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.push_to_hub(\"javilonso/classificationEsp1_TitleWithOpinion_Polarity\")\n",
    "# tokenizer.push_to_hub(\"javilonso/classificationEsp1_TitleWithOpinion_Polarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at javilonso/classificationEsp1_Attraction.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "Loading cached processed dataset at ../data/train/cache-1f28644c3516951f.arrow\n",
      "Loading cached processed dataset at ../data/test/cache-82babf341eaded68.arrow\n"
     ]
    }
   ],
   "source": [
    "# # Tests ------\n",
    "\n",
    "# check_point = \"javilonso/classificationEsp1_Augmented_Polarity\"\n",
    "\n",
    "# from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "# model = TFAutoModelForSequenceClassification.from_pretrained(check_point)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(check_point)\n",
    "\n",
    "\n",
    "# from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# raw_dataset = DatasetDict.load_from_disk(\".././data\")\n",
    "\n",
    "# def tokenize(example):\n",
    "#     tokenized_example = tokenizer(example[\"Opinion\"], truncation=True)\n",
    "#     tokenized_example[\"label\"] = example[\"Polarity\"]\n",
    "#     return tokenized_example\n",
    "\n",
    "\n",
    "# tokenized_dataset = raw_dataset.map(tokenize, batched=True, remove_columns=raw_dataset[\"train\"].column_names)\n",
    "\n",
    "\n",
    "# from transformers import DataCollatorWithPadding\n",
    "\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "\n",
    "\n",
    "# tf_train_dataset = tokenized_dataset[\"train\"].to_tf_dataset(\n",
    "#     columns=[\"attenti on_mask\", \"input_ids\", \"label\"],\n",
    "#     collate_fn=data_collator,\n",
    "#     shuffle=True,\n",
    "#     batch_size=16,\n",
    "# )\n",
    "\n",
    "# tf_eval_dataset = tokenized_dataset[\"test\"].to_tf_dataset(\n",
    "#     columns=[\"attention_mask\", \"input_ids\", \"label\"],\n",
    "#     collate_fn=data_collator,\n",
    "#     shuffle=False,\n",
    "#     batch_size=16,\n",
    "# )\n",
    "\n",
    "# import numpy as np\n",
    "# from datasets import load_metric\n",
    " \n",
    "# load_accuracy = load_metric(\"accuracy\")\n",
    "# load_f1 = load_metric(\"f1\")\n",
    "# for batch in tf_eval_dataset:\n",
    "#     logits = model.predict(batch)[\"logits\"]\n",
    "#     labels = batch[\"labels\"]\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     load_accuracy.add_batch(predictions=predictions, references=labels)\n",
    "#     load_f1.add_batch(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "# accuracy = load_accuracy.compute()[\"accuracy\"]\n",
    "# f1 = load_f1.compute(average=None)[\"f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9883\n",
      "F1: [0.99079755 0.97993411 0.99385343]\n"
     ]
    }
   ],
   "source": [
    "# print(f'Accuracy: {accuracy:.4f}')\n",
    "# print(f'F1: {f1}')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6a64f2c38f9968623135a8fad9c1eee06acf613206d3ab9bd941bde9bf30840"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
